{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhrmBtsUdtnBiKowz0Jq3W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaliniAnandaPhD/Real-Time-News-Aggregator-and-recommender/blob/main/Real_time_news_aggregator_and_recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data collection"
      ],
      "metadata": {
        "id": "Pm0-ZlO7LKeK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4M-RlLrKvcp",
        "outputId": "4776627e-1ffd-4214-c38d-b484daeaa3c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                   source              author  \\\n",
            "0  {'id': 'engadget', 'name': 'Engadget'}         Malak Saleh   \n",
            "1        {'id': 'wired', 'name': 'Wired'}  Angela Watercutter   \n",
            "2        {'id': 'wired', 'name': 'Wired'}     Tristan Kennedy   \n",
            "3        {'id': 'wired', 'name': 'Wired'}         Alex Miller   \n",
            "4        {'id': 'wired', 'name': 'Wired'}  Caitlin Harrington   \n",
            "\n",
            "                                               title  \\\n",
            "0  Researchers developed a gene-editing technolog...   \n",
            "1  \"Now and Then,\" The Beatles’ Last Song, Is Her...   \n",
            "2                          Skiing Is Getting Riskier   \n",
            "3         Tech and Games Can Help Curb Youth Suicide   \n",
            "4  This AI Bot Fills Out Job Applications for You...   \n",
            "\n",
            "                                         description  \\\n",
            "0  In a trial run by Verve Therapeutics\\r\\n, a Ca...   \n",
            "1  The track was made possible thanks to technolo...   \n",
            "2  The threat of avalanches is rising with global...   \n",
            "3  In the face of lackluster mental health suppor...   \n",
            "4  AI-powered services like LazyApply zip through...   \n",
            "\n",
            "                                                 url  \\\n",
            "0  https://www.engadget.com/researchers-developed...   \n",
            "1  https://www.wired.com/story/the-beatles-now-an...   \n",
            "2  https://www.wired.com/story/avalanche-threat-s...   \n",
            "3  https://www.wired.com/story/tech-video-games-y...   \n",
            "4  https://www.wired.com/story/this-ai-bot-fills-...   \n",
            "\n",
            "                                          urlToImage           publishedAt  \\\n",
            "0  https://s.yimg.com/ny/api/res/1.2/S7qYWIbbLiVo...  2023-11-16T17:00:40Z   \n",
            "1  https://media.wired.com/photos/65451021c7c6de7...  2023-11-03T17:46:51Z   \n",
            "2  https://media.wired.com/photos/654ece1ce8bd90f...  2023-11-11T12:00:00Z   \n",
            "3  https://media.wired.com/photos/65452a95ae2b61a...  2023-11-07T12:00:00Z   \n",
            "4  https://media.wired.com/photos/6545939ac24c848...  2023-11-06T12:00:00Z   \n",
            "\n",
            "                                             content  \n",
            "0  In a trial run by Verve Therapeutics\\r\\n, a Ca...  \n",
            "1  Following a lot of hypeand a quarter-century o...  \n",
            "2  As Olivier Gardet piloted the drone around the...  \n",
            "3  One of my most traumatizing moments was when m...  \n",
            "4  In July, software engineer Julian Joseph becam...  \n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "def fetch_news(api_key, query='technology', page_size=30, page=1):\n",
        "    base_url = 'https://newsapi.org/v2/everything'\n",
        "    params = {\n",
        "        'q': query,\n",
        "        'apiKey': api_key,\n",
        "        'pageSize': page_size,\n",
        "        'page': page\n",
        "    }\n",
        "    response = requests.get(base_url, params=params)\n",
        "    return response.json()\n",
        "\n",
        "# Use your API key here\n",
        "api_key = '59205caa54af4966a522adf166e8e8a0'\n",
        "news_data = fetch_news(api_key)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(news_data['articles'])\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preprocessing"
      ],
      "metadata": {
        "id": "TKWl1SQyLUDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove HTML tags, non-alphanumeric characters, and convert to lowercase\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove stopwords and perform lemmatization\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply cleaning to the DataFrame\n",
        "df['content_cleaned'] = df['content'].apply(clean_text)\n",
        "print(df[['content', 'content_cleaned']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B19bWk_vLWg3",
        "outputId": "ebb4cc58-703e-4853-f123-425a854e89ca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             content  \\\n",
            "0  In a trial run by Verve Therapeutics\\r\\n, a Ca...   \n",
            "1  Following a lot of hypeand a quarter-century o...   \n",
            "2  As Olivier Gardet piloted the drone around the...   \n",
            "3  One of my most traumatizing moments was when m...   \n",
            "4  In July, software engineer Julian Joseph becam...   \n",
            "\n",
            "                                     content_cleaned  \n",
            "0  trial run verve therapeutic cambridgebased bio...  \n",
            "1  following lot hypeand quartercentury worknow p...  \n",
            "2  olivier gardet piloted drone around mountain c...  \n",
            "3  one traumatizing moment best friend terry got ...  \n",
            "4  july software engineer julian joseph became la...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "News Categorization"
      ],
      "metadata": {
        "id": "hesgz5rCLaV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import movie_reviews\n",
        "from random import shuffle\n",
        "from nltk import FreqDist, NaiveBayesClassifier\n",
        "from nltk.classify import accuracy\n",
        "\n",
        "nltk.download('movie_reviews')\n",
        "\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "shuffle(documents)\n",
        "\n",
        "all_words = FreqDist(word.lower() for word in movie_reviews.words())\n",
        "word_features = list(all_words)[:2000]\n",
        "\n",
        "def document_features(document):\n",
        "    document_words = set(document)\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features[f'contains({word})'] = (word in document_words)\n",
        "    return features\n",
        "\n",
        "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
        "train_set, test_set = featuresets[100:], featuresets[:100]\n",
        "classifier = NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "# Example prediction\n",
        "print(classifier.classify(document_features(df['content_cleaned'][0].split())))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3eth2bLLdB2",
        "outputId": "b25f1f05-2911-4858-db9e-a8c61578e347"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "User Interaction and Data Collection - here is flask and streamlit!"
      ],
      "metadata": {
        "id": "EhavHfOwLRGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/read_article', methods=['POST'])\n",
        "def read_article():\n",
        "    user_id = request.json['user_id']\n",
        "    article_id = request.json['article_id']\n",
        "    # Here, you'd record the user's reading history\n",
        "    return jsonify({'status': 'success'})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwMGrpr2Ln2t",
        "outputId": "f2a08f30-cf12-4fe6-f716-9bbb589cd92c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also try streamlit"
      ],
      "metadata": {
        "id": "CwcosTQAMOEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjxj_jN8MQjd",
        "outputId": "24fe834a-de63-4511-e8ee-ff44a1e65734"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.28.2-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: importlib-metadata<7,>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.8.0)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.23.5)\n",
            "Requirement already satisfied: packaging<24,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (23.2)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.5.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=6.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.0)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.2.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.5.0)\n",
            "Requirement already satisfied: tzlocal<6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.2)\n",
            "Collecting validators<1,>=0.2 (from streamlit)\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.2)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7,>=1.4->streamlit) (3.17.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2023.7.22)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.11.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.31.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.13.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Installing collected packages: watchdog, validators, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.40 pydeck-0.8.1b0 smmap-5.0.1 streamlit-1.28.2 validators-0.22.0 watchdog-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile streamlit_app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "\n",
        "# Sample DataFrame with news articles\n",
        "# In a real application, this would be replaced with your actual data\n",
        "articles = pd.DataFrame({\n",
        "    'article_id': range(1, 6),\n",
        "    'title': ['Article 1', 'Article 2', 'Article 3', 'Article 4', 'Article 5'],\n",
        "    'content': ['Content 1', 'Content 2', 'Content 3', 'Content 4', 'Content 5']\n",
        "})\n",
        "\n",
        "# Simulate a user_id (in a real app, this would come from your user management system)\n",
        "user_id = 1\n",
        "\n",
        "# Streamlit interface\n",
        "st.title(\"News Recommender System\")\n",
        "\n",
        "# Display articles\n",
        "for index, row in articles.iterrows():\n",
        "    st.subheader(row['title'])\n",
        "    st.write(row['content'])\n",
        "\n",
        "    # Button to record reading an article\n",
        "    if st.button(f'Read Article {row[\"article_id\"]}'):\n",
        "        # Here, record the user's reading history\n",
        "        # For example, append to a file or a database\n",
        "        # This is a placeholder for the logic you'd implement\n",
        "        st.write(f\"You read article {row['article_id']}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_lmAd5AMUVk",
        "outputId": "ac2d761a-ebf7-4657-e7c8-12ec3d845d66"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing streamlit_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recommendation System"
      ],
      "metadata": {
        "id": "kW9qwdoFMsQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Ensure that the DataFrame has the required columns\n",
        "if 'content_cleaned' in df.columns and 'title' in df.columns:\n",
        "    # Check and remove any rows with missing or empty values in 'content_cleaned' or 'title'\n",
        "    df = df.dropna(subset=['content_cleaned', 'title'])\n",
        "    df = df[df['content_cleaned'].str.strip() != '']\n",
        "\n",
        "    # Creating TF-IDF matrix\n",
        "    tfidf = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_matrix = tfidf.fit_transform(df['content_cleaned'])\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "    def recommend_articles(title, cosine_sim=cosine_sim):\n",
        "        if title in df['title'].values:\n",
        "            idx = df[df['title'] == title].index[0]\n",
        "            sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "            sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "            sim_scores = sim_scores[1:11]  # Get top 10 recommendations\n",
        "            article_indices = [i[0] for i in sim_scores]\n",
        "            return df['title'].iloc[article_indices]\n",
        "        else:\n",
        "            return f\"Title '{title}' not found in the DataFrame.\"\n",
        "\n",
        "    # Display some titles from the DataFrame\n",
        "    print(\"Some titles from the DataFrame:\")\n",
        "    print(df['title'].head())\n",
        "\n",
        "    # Example recommendation\n",
        "    try:\n",
        "        # Replace with an actual title from your DataFrame\n",
        "        sample_title = df['title'].iloc[0]  # using the first title in the DataFrame\n",
        "        print(f\"Recommendations for '{sample_title}':\")\n",
        "        print(recommend_articles(sample_title))\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "else:\n",
        "    print(\"DataFrame does not have the required columns 'content_cleaned' and 'title'.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj8uaUM3Mt1r",
        "outputId": "5b70c7df-6fa9-41b7-a41e-3c1c6c0900ab"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some titles from the DataFrame:\n",
            "0    Researchers developed a gene-editing technolog...\n",
            "1    \"Now and Then,\" The Beatles’ Last Song, Is Her...\n",
            "2                            Skiing Is Getting Riskier\n",
            "3           Tech and Games Can Help Curb Youth Suicide\n",
            "4    This AI Bot Fills Out Job Applications for You...\n",
            "Name: title, dtype: object\n",
            "Recommendations for 'Researchers developed a gene-editing technology that reduces 'bad' cholesterol':\n",
            "11    Researchers printed a robotic hand with bones,...\n",
            "8              Is AI the answer to sustainable farming?\n",
            "10    Swedish court clears dual national Skvortsov o...\n",
            "16    OpenAI CEO Sam Altman ousted as 'board no long...\n",
            "22    Is technology our savior — or our slayer? | Ru...\n",
            "3            Tech and Games Can Help Curb Youth Suicide\n",
            "21    These Solid-State Drivers Bring the Future of ...\n",
            "9                 The Case for Buying ‘Dumb’ Appliances\n",
            "1     \"Now and Then,\" The Beatles’ Last Song, Is Her...\n",
            "25    GM’s Cruise Halts Self-Driving Operations Acro...\n",
            "Name: title, dtype: object\n"
          ]
        }
      ]
    }
  ]
}